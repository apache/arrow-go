// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package file

import (
    "github.com/apache/arrow-go/v18/parquet"
    "github.com/apache/arrow-go/v18/parquet/internal/encoding"
)

{{range .In}}
// {{.Name}}ColumnChunkReader is the Typed Column chunk reader instance for reading
// {{.Name}} column data.
type {{.Name}}ColumnChunkReader struct {
  columnChunkReader
}

// Skip skips the next nvalues so that the next call to ReadBatch
// will start reading *after* the skipped values.
func (cr *{{.Name}}ColumnChunkReader) Skip(nvalues int64) (int64, error) {
  err := cr.columnChunkReader.skipRows(nvalues)
  return nvalues, err
}

{{- if .isByteArray}}
// ReadBatchInPage and ReadBatch are used to read batchSize values from the column, currently they are 
// used in test. The former one only reads values from the same page, and might be shallow copies of the
// underlying page data, while the latter one reads across multiple pages to fill the batch and values
// have been cloned. User should choose the appropriate one based on their use cases. For example, if user
// only needs to read and process values one by one, ReadBatchInPage is more efficient. Otherwise, if user
// want to cache the values for later use, ReadBatch is more suitable.
{{- else}}
// ReadBatchInPage and ReadBatch are used to read batchSize values from the column, currently they are 
// used in test. The former one only reads values from the same page, while the latter one reads across
// multiple pages to fill the batch.
{{end}}
//
// Returns error if values is not at least big enough to hold the number of values that will be read.
//
// defLvls and repLvls can be nil, or will be populated if not nil. If not nil, they must be
// at least large enough to hold the number of values that will be read.
//
// total is the number of rows that were read, valuesRead is the actual number of physical values
// that were read excluding nulls
func (cr *{{.Name}}ColumnChunkReader) ReadBatchInPage(batchSize int64, values []{{.name}}, defLvls, repLvls []int16) (total int64, valuesRead int, err error) {
  return cr.readBatchInPage(batchSize, 0, defLvls, repLvls, func(start, len int64) (int, error) {
    return cr.curDecoder.(encoding.{{.Name}}Decoder).Decode(values[start:start+len])
  })
}

func (cr *{{.Name}}ColumnChunkReader) ReadBatch(batchSize int64, values []{{.name}}, defLvls, repLvls []int16) (total int64, valuesRead int, err error) {
  return cr.readBatch(batchSize, defLvls, repLvls, func(start, len int64) (int, error) {
{{- if .isByteArray}}
    n, err := cr.curDecoder.(encoding.{{.Name}}Decoder).Decode(values[start:start+len])
    if err == nil {
      cloneByteArray(values[start : start+len])
    }
    return n, err
{{- else}}
    return cr.curDecoder.(encoding.{{.Name}}Decoder).Decode(values[start : start+len])
{{- end}}
  })
}
{{end}}